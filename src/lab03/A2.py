from re import *
def tokenize(text):
    pattern = (r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]+([-][a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]+)*')
    tokens = []
    for match in finditer(pattern,text):
        tokens.append(match.group())
    return tokens
test_cases = [
    "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "hello,world!!!",
    "no-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", 
    "2025 –≥–æ–¥",
    "emoji üí¨ –Ω–µ —Å–ª–æ–≤–æ"
]
for text in test_cases:
    result = tokenize(text)
    print(result)